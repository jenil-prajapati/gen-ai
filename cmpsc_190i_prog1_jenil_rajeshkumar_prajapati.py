# -*- coding: utf-8 -*-
"""CMPSC_190I - PROG1 - Jenil Rajeshkumar Prajapati - Final Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AWj4ftO5ivZdwqpPK4OWY1857JVR4Nmk
"""

!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar
!tar -xf VOCtrainval_11-May-2012.tar

!pip install albumentations opencv-python-headless pyyaml tqdm
!pip install torch torchvision

config = {
    'dataset_params': {
        'train_im_sets': ['VOCdevkit/VOC2012'],
        'test_im_sets': ['VOCdevkit/VOC2012'],
        'im_size': 448,
        'num_classes': 20
    },
    'model_params': {
        'im_channels': 3,
        'backbone_channels': 512,
        'yolo_conv_channels': 512,
        'conv_spatial_size': 7,
        'fc_dim': 4096,
        'fc_dropout': 0.5,
        'leaky_relu_slope': 0.1,
        'use_conv': False,
        'S': 7,
        'B': 2,
        'use_sigmoid': True
    },
    'train_params': {
        'task_name': 'checkpoints',
        'ckpt_name': 'yolov1.pth',
        'num_epochs': 50,
        'batch_size': 8,
        'acc_steps': 1,
        "lr": 1e-4,
        'lr_steps': [1],
        'log_steps': 1,
        'infer_conf_threshold': 0.3,
        'eval_conf_threshold': 0.3,
        'nms_threshold': 0.4,
        'seed': 42
    }
}

import torch
import torch.nn as nn
import torchvision

class YOLOV1(nn.Module):
    def __init__(self, im_size, num_classes, model_config):
        super(YOLOV1, self).__init__()
        self.im_size = im_size
        self.im_channels = model_config['im_channels']
        self.backbone_channels = model_config['backbone_channels']
        self.yolo_conv_channels = model_config['yolo_conv_channels']
        self.conv_spatial_size = model_config['conv_spatial_size']
        self.leaky_relu_slope = model_config['leaky_relu_slope']
        self.yolo_fc_hidden_dim = model_config['fc_dim']
        self.yolo_fc_dropout_prob = model_config['fc_dropout']
        self.use_conv = model_config['use_conv']
        self.S = model_config['S']
        self.B = model_config['B']
        self.C = num_classes

        backbone = torchvision.models.resnet34(
            weights=torchvision.models.ResNet34_Weights.IMAGENET1K_V1
        )
        ###################
        # Backbone Layers #
        ###################
        self.features = nn.Sequential(
            backbone.conv1,
            backbone.bn1,
            backbone.relu,
            backbone.maxpool,
            backbone.layer1,
            backbone.layer2,
            backbone.layer3,
            backbone.layer4,
        )

        #########################
        # Detection Conv Layers #
        #########################
        self.conv_yolo_layers = nn.Sequential(
            nn.Conv2d(self.backbone_channels,
                      self.yolo_conv_channels,
                      3,
                      padding=1,
                      bias=False),
            nn.BatchNorm2d(self.yolo_conv_channels),
            nn.LeakyReLU(self.leaky_relu_slope),
            nn.Conv2d(self.yolo_conv_channels,
                      self.yolo_conv_channels,
                      3,
                      stride=2,
                      padding=1,
                      bias=False),
            nn.BatchNorm2d(self.yolo_conv_channels),
            nn.LeakyReLU(self.leaky_relu_slope),
            nn.Conv2d(self.yolo_conv_channels,
                      self.yolo_conv_channels,
                      3,
                      padding=1,
                      bias=False),
            nn.BatchNorm2d(self.yolo_conv_channels),
            nn.LeakyReLU(self.leaky_relu_slope),
            nn.Conv2d(self.yolo_conv_channels,
                      self.yolo_conv_channels,
                      3,
                      padding=1,
                      bias=False),
            nn.BatchNorm2d(self.yolo_conv_channels),
            nn.LeakyReLU(self.leaky_relu_slope)
            )

        #######################
        # Detection Layers #
        #######################
        if self.use_conv:
            self.fc_yolo_layers = nn.Sequential(
                nn.Conv2d(self.yolo_conv_channels, 5 * self.B + self.C, 1),
            )
        else:
            self.fc_yolo_layers = nn.Sequential(
                nn.Flatten(),
                nn.Linear(self.conv_spatial_size * self.conv_spatial_size *
                          self.yolo_conv_channels,
                          self.yolo_fc_hidden_dim),
                nn.LeakyReLU(self.leaky_relu_slope),
                nn.Dropout(self.yolo_fc_dropout_prob),
                nn.Linear(self.yolo_fc_hidden_dim,
                          self.S * self.S * (5 * self.B + self.C)),
            )
    def forward(self, x):
        out = self.features(x)
        out = self.conv_yolo_layers(out)
        out = self.fc_yolo_layers(out)
        if self.use_conv:
            out = out.permute(0, 2, 3, 1)
        return out

def get_iou(boxes1, boxes2):
    # IOU between two sets of boxes
    area1 = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])
    area2 = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])

    x_left = torch.max(boxes1[..., 0], boxes2[..., 0])
    y_top = torch.max(boxes1[..., 1], boxes2[..., 1])
    x_right = torch.min(boxes1[..., 2], boxes2[..., 2])
    y_bottom = torch.min(boxes1[..., 3], boxes2[..., 3])

    intersection_area = (x_right - x_left).clamp(min=0) * (y_bottom - y_top).clamp(min=0)
    union = area1.clamp(min=0) + area2.clamp(min=0) - intersection_area
    iou = intersection_area / (union + 1E-6)
    return iou

class YOLOV1Loss(nn.Module):
    def __init__(self, S=7, B=2, C=20):
        super(YOLOV1Loss, self).__init__()
        self.S = S
        self.B = B
        self.C = C
        self.lambda_coord = 5
        self.lambda_noobj = 0.5

    def forward(self, preds, targets, use_sigmoid=False):
        batch_size = preds.size(0)


        preds = preds.reshape(batch_size, self.S, self.S, 5*self.B + self.C)

        if use_sigmoid:
            preds[..., :5 * self.B] = torch.nn.functional.sigmoid(preds[..., :5 * self.B])


        shifts_x = torch.arange(0, self.S,
                                dtype=torch.int32,
                                device=preds.device) * 1 / float(self.S)
        shifts_y = torch.arange(0, self.S,
                                dtype=torch.int32,
                                device=preds.device) * 1 / float(self.S)

        shifts_y, shifts_x = torch.meshgrid(shifts_y, shifts_x, indexing="ij")

        shifts_x = shifts_x.reshape((1, self.S, self.S, 1)).repeat(1, 1, 1, self.B)
        shifts_y = shifts_y.reshape((1, self.S, self.S, 1)).repeat(1, 1, 1, self.B)

        pred_boxes = preds[..., :5*self.B].reshape(batch_size,
                                                   self.S,
                                                   self.S,
                                                   self.B,
                                                   -1)


        pred_boxes_x1 = ((pred_boxes[..., 0]/self.S + shifts_x)
                         - 0.5*torch.square(pred_boxes[..., 2]))
        pred_boxes_x1 = pred_boxes_x1[..., None]
        pred_boxes_y1 = ((pred_boxes[..., 1]/self.S + shifts_y)
                         - 0.5*torch.square(pred_boxes[..., 3]))
        pred_boxes_y1 = pred_boxes_y1[..., None]
        pred_boxes_x2 = ((pred_boxes[..., 0]/self.S + shifts_x)
                         + 0.5*torch.square(pred_boxes[..., 2]))
        pred_boxes_x2 = pred_boxes_x2[..., None]
        pred_boxes_y2 = ((pred_boxes[..., 1]/self.S + shifts_y)
                         + 0.5*torch.square(pred_boxes[..., 3]))
        pred_boxes_y2 = pred_boxes_y2[..., None]
        pred_boxes_x1y1x2y2 = torch.cat([
            pred_boxes_x1,
            pred_boxes_y1,
            pred_boxes_x2,
            pred_boxes_y2], dim=-1)

        target_boxes = targets[..., :5*self.B].reshape(batch_size,
                                                       self.S,
                                                       self.S,
                                                       self.B,
                                                       -1)
        target_boxes_x1 = ((target_boxes[..., 0] / self.S + shifts_x)
                           - 0.5 * torch.square(target_boxes[..., 2]))
        target_boxes_x1 = target_boxes_x1[..., None]
        target_boxes_y1 = ((target_boxes[..., 1] / self.S + shifts_y)
                           - 0.5 * torch.square(target_boxes[..., 3]))
        target_boxes_y1 = target_boxes_y1[..., None]
        target_boxes_x2 = ((target_boxes[..., 0] / self.S + shifts_x)
                           + 0.5 * torch.square(target_boxes[..., 2]))
        target_boxes_x2 = target_boxes_x2[..., None]
        target_boxes_y2 = ((target_boxes[..., 1] / self.S + shifts_y)
                           + 0.5 * torch.square(target_boxes[..., 3]))
        target_boxes_y2 = target_boxes_y2[..., None]
        target_boxes_x1y1x2y2 = torch.cat([
            target_boxes_x1,
            target_boxes_y1,
            target_boxes_x2,
            target_boxes_y2
        ], dim=-1)

        iou = get_iou(pred_boxes_x1y1x2y2, target_boxes_x1y1x2y2)

        max_iou_val, max_iou_idx = iou.max(dim=-1, keepdim=True)

        #########################
        # Indicator Definitions #
        #########################

        max_iou_idx = max_iou_idx.repeat(1, 1, 1, self.B)
        bb_idxs = (torch.arange(self.B).reshape(1, 1, 1, self.B).expand_as(max_iou_idx)
                   .to(preds.device))

        is_max_iou_box = (max_iou_idx == bb_idxs).long()


        obj_indicator = targets[..., 4:5]


        #######################
        # Classification Loss #
        #######################
        cls_target = targets[..., 5 * self.B:]
        cls_preds = preds[..., 5 * self.B:]
        cls_mse = (cls_preds - cls_target) ** 2

        cls_mse = (obj_indicator * cls_mse).sum()

        ######################################################
        # Objectness Loss (For responsible predictor boxes ) #
        ######################################################

        is_max_box_obj_indicator = is_max_iou_box * obj_indicator
        obj_mse = (pred_boxes[..., 4] - max_iou_val) ** 2

        obj_mse = (is_max_box_obj_indicator * obj_mse).sum()

        #####################
        # Localization Loss #
        #####################
        x_mse = (pred_boxes[..., 0] - target_boxes[..., 0]) ** 2

        x_mse = (is_max_box_obj_indicator * x_mse).sum()

        y_mse = (pred_boxes[..., 1] - target_boxes[..., 1]) ** 2
        y_mse = (is_max_box_obj_indicator * y_mse).sum()
        w_sqrt_mse = (pred_boxes[..., 2] - target_boxes[..., 2]) ** 2
        w_sqrt_mse = (is_max_box_obj_indicator * w_sqrt_mse).sum()
        h_sqrt_mse = (pred_boxes[..., 3] - target_boxes[..., 3]) ** 2
        h_sqrt_mse = (is_max_box_obj_indicator * h_sqrt_mse).sum()

        #################################################
        # Objectness Loss
        # For boxes of cells assigned with object that
        # aren't responsible predictor boxes
        # and for boxes of cell not assigned with object
        #################################################
        no_object_indicator = 1 - is_max_box_obj_indicator
        no_obj_mse = (pred_boxes[..., 4] - torch.zeros_like(pred_boxes[..., 4])) ** 2
        no_obj_mse = (no_object_indicator * no_obj_mse).sum()

        ##############
        # Total Loss #
        ##############
        loss = self.lambda_coord*(x_mse + y_mse + w_sqrt_mse + h_sqrt_mse)
        loss += cls_mse + obj_mse
        loss += self.lambda_noobj*no_obj_mse
        loss = loss / batch_size
        return loss

import os
import random

# Set seed for reproducibility
random.seed(42)

# Paths
main_dir = 'VOCdevkit/VOC2012/ImageSets/Main'
trainval_path = os.path.join(main_dir, 'trainval.txt')

# Read image IDs
with open(trainval_path, 'r') as f:
    image_ids = f.read().strip().splitlines()

# Shuffle and split 80/20
random.shuffle(image_ids)
split_idx = int(0.8 * len(image_ids))
train_ids = image_ids[:split_idx]
val_ids = image_ids[split_idx:]

# Write to new files
with open(os.path.join(main_dir, 'train_custom.txt'), 'w') as f:
    f.write('\n'.join(train_ids))

with open(os.path.join(main_dir, 'val_custom.txt'), 'w') as f:
    f.write('\n'.join(val_ids))

print("âœ… train_custom.txt and val_custom.txt created!")

import cv2
import albumentations as albu
import torch
import xml.etree.ElementTree as ET
from torch.utils.data import Dataset

def parse_voc_annotations(im_sets, label2idx, ann_fname, split):
    import os
    import xml.etree.ElementTree as ET

    im_infos = []
    for im_set in im_sets:
        im_names = []
        for line in open(os.path.join(im_set, 'ImageSets', 'Main', f'{ann_fname}.txt')):
            im_names.append(line.strip())

        ann_dir = os.path.join(im_set, 'Annotations')
        im_dir = os.path.join(im_set, 'JPEGImages')

        for im_name in im_names:
            ann_file = os.path.join(ann_dir, f'{im_name}.xml')
            im_info = {}
            ann_info = ET.parse(ann_file)
            root = ann_info.getroot()
            size = root.find('size')
            width = int(size.find('width').text)
            height = int(size.find('height').text)
            im_info['img_id'] = os.path.basename(ann_file).split('.xml')[0]
            im_info['filename'] = os.path.join(im_dir, f'{im_info["img_id"]}.jpg')
            im_info['width'] = width
            im_info['height'] = height
            detections = []

            any_valid_object = False
            for obj in ann_info.findall('object'):
                det = {}
                label = label2idx[obj.find('name').text]
                difficult = int(obj.find('difficult').text)
                bbox_info = obj.find('bndbox')
                bbox = [
                    int(float(bbox_info.find('xmin').text)) - 1,
                    int(float(bbox_info.find('ymin').text)) - 1,
                    int(float(bbox_info.find('xmax').text)) - 1,
                    int(float(bbox_info.find('ymax').text)) - 1
                ]
                det['label'] = label
                det['bbox'] = bbox
                det['difficult'] = difficult
                if difficult == 0 or split == 'test':
                    detections.append(det)
                    any_valid_object = True

            if any_valid_object:
                im_info['detections'] = detections
                im_infos.append(im_info)

    print(f'Total {len(im_infos)} images found')
    return im_infos


class VOCDataset(Dataset):
    def __init__(self, split, im_sets, im_size=448, S=7, B=2, C=20):
        self.split = split
        self.im_sets = im_sets
        #self.fname = 'trainval' if self.split == 'train' else 'test'
        #self.fname = 'trainval'
        self.fname = 'train_custom' if split == 'train' else 'val_custom'
        self.im_size = im_size
        self.S = S
        self.B = B
        self.C = C

        self.transforms = {
            'train': albu.Compose([
                albu.HorizontalFlip(p=0.5),
                albu.Affine(
                    scale=(0.8, 1.2),
                    translate_percent=(-0.2, 0.2),
                    always_apply=True
                ),
                albu.ColorJitter(
                    brightness=(0.8, 1.2),
                    contrast=(0.8, 1.2),
                    saturation=(0.8, 1.2),
                    hue=(-0.2, 0.2),
                    always_apply=None,
                    p=0.5,
                ),
                albu.Resize(self.im_size, self.im_size)],
                bbox_params=albu.BboxParams(format='pascal_voc',
                                            label_fields=['labels'])),
            'test': albu.Compose([
                albu.Resize(self.im_size, self.im_size),
                ],
                bbox_params=albu.BboxParams(format='pascal_voc',
                                            label_fields=['labels']))
        }

        classes = [
            'person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',
            'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',
            'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor'
        ]
        classes = sorted(classes)
        self.label2idx = {classes[idx]: idx for idx in range(len(classes))}
        self.idx2label = {idx: classes[idx] for idx in range(len(classes))}
        print(self.idx2label)
        self.images_info = parse_voc_annotations(self.im_sets,
                                                self.label2idx,
                                                self.fname,
                                                self.split)


    def __len__(self):
        return len(self.images_info)

    def __getitem__(self, index):
        im_info = self.images_info[index]
        im = cv2.imread(im_info['filename'])
        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)

        bboxes = [detection['bbox'] for detection in im_info['detections']]
        labels = [detection['label'] for detection in im_info['detections']]
        difficult = [detection['difficult'] for detection in im_info['detections']]

        transformed_info = self.transforms[self.split](image=im,
                                                       bboxes=bboxes,
                                                       labels=labels)
        im = transformed_info['image']
        bboxes = torch.as_tensor(transformed_info['bboxes'])
        labels = torch.as_tensor(transformed_info['labels'])
        difficult = torch.as_tensor(difficult)

        im_tensor = torch.from_numpy(im / 255.).permute((2, 0, 1)).float()
        im_tensor_channel_0 = (torch.unsqueeze(im_tensor[0], 0) - 0.485) / 0.229
        im_tensor_channel_1 = (torch.unsqueeze(im_tensor[1], 0) - 0.456) / 0.224
        im_tensor_channel_2 = (torch.unsqueeze(im_tensor[2], 0) - 0.406) / 0.225
        im_tensor = torch.cat((im_tensor_channel_0,
                               im_tensor_channel_1,
                               im_tensor_channel_2), 0)
        bboxes_tensor = torch.as_tensor(bboxes)
        labels_tensor = torch.as_tensor(labels)

        target_dim = 5 * self.B + self.C
        h, w = im.shape[:2]
        yolo_targets = torch.zeros(self.S, self.S, target_dim)

        cell_pixels = h // self.S

        if len(bboxes) > 0:

            box_widths = bboxes_tensor[:, 2] - bboxes_tensor[:, 0]
            box_heights = bboxes_tensor[:, 3] - bboxes_tensor[:, 1]
            box_center_x = bboxes_tensor[:, 0] + 0.5 * box_widths
            box_center_y = bboxes_tensor[:, 1] + 0.5 * box_heights


            box_i = torch.floor(box_center_x / cell_pixels).long()
            box_j = torch.floor(box_center_y / cell_pixels).long()


            box_xc_cell_offset = (box_center_x - box_i*cell_pixels) / cell_pixels
            box_yc_cell_offset = (box_center_y - box_j*cell_pixels) / cell_pixels


            box_w_label = box_widths / w
            box_h_label = box_heights / h


            for idx, b in enumerate(range(bboxes_tensor.size(0))):

                for k in range(self.B):
                    s = 5 * k

                    yolo_targets[box_j[idx], box_i[idx], s] = box_xc_cell_offset[idx]
                    yolo_targets[box_j[idx], box_i[idx], s+1] = box_yc_cell_offset[idx]
                    yolo_targets[box_j[idx], box_i[idx], s+2] = box_w_label[idx].sqrt()
                    yolo_targets[box_j[idx], box_i[idx], s+3] = box_h_label[idx].sqrt()
                    yolo_targets[box_j[idx], box_i[idx], s+4] = 1.0
                label = int(labels[b])
                cls_target = torch.zeros((self.C,))
                cls_target[label] = 1.
                yolo_targets[box_j[idx], box_i[idx], 5 * self.B:] = cls_target

        if len(bboxes) > 0:
            bboxes_tensor /= torch.Tensor([[w, h, w, h]]).expand_as(bboxes_tensor)
        targets = {
            'bboxes': bboxes_tensor,
            'labels': labels_tensor,
            'yolo_targets': yolo_targets,
            'difficult': difficult,
        }
        return im_tensor, targets, im_info['filename']

import cv2
import numpy as np
from tqdm import tqdm

COLORS = [[0, 0, 0], [200, 0, 0], [0, 200, 0], [50, 128, 0], [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128], [50, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0], [64, 0, 128], [192, 0, 128], [50, 128, 200], [192, 128, 200], [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0], [0, 64, 128]]


def draw_bounding_box(img, bbox, class_name, score=None, color=(255, 0, 0), thickness=2):
    r"""
    Draws a single bounding box on image
    """
    x_min, y_min, x_max, y_max = bbox
    x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max )
    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)
    box_text = class_name + ' {:.2f}'.format(score) if score is not None else class_name
    ((text_width, text_height), _) = cv2.getTextSize(box_text, cv2.FONT_HERSHEY_SIMPLEX, 0.45, 1)
    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), color, -1)
    cv2.putText(
        img,
        text=box_text,
        org=(x_min, y_min - int(0.3 * text_height)),
        fontFace=cv2.FONT_HERSHEY_SIMPLEX,
        fontScale=0.45,
        color=(255, 255, 255),
        lineType=cv2.LINE_AA,
    )
    return img


def visualize(image, bboxes, category_ids, category_id_to_name, scores=None):
    img = image.copy()
    for idx, (bbox, category_id) in enumerate(zip(bboxes, category_ids)):
        class_name = category_id_to_name[category_id]
        img = draw_bounding_box(img, bbox, class_name, scores[idx] if scores is not None else None)
    return img


# def overlay_grid(img, grid_shape, color=(0, 0, 0), thickness=2):
#     r"""
#     Draws a grid on image
#     """
#     grid_im = np.copy(img)
#     h, w, _ = grid_im.shape
#     rows, cols = grid_shape
#     dy, dx = h / rows, w / cols

#     # draw vertical lines
#     for x in np.linspace(start=dx, stop=w-dx, num=cols-1):
#         x = int(round(x))
#         cv2.line(grid_im, (x, 0), (x, h), color=color, thickness=thickness)

#     # draw horizontal lines
#     for y in np.linspace(start=dy, stop=h-dy, num=rows-1):
#         y = int(round(y))
#         cv2.line(grid_im, (0, y), (w, y), color=color, thickness=thickness)

#     return grid_im


# def color_grid_by_class(img, cls_idx, grid_shape):
#     r"""
#     Draws color coded grid for the entire image
#     coded based on the class label
#     """
#     rect_im = np.copy(img)
#     h, w, _ = rect_im.shape
#     rows, cols = grid_shape
#     dy, dx = h / rows, w / cols
#     for i in range(rows):
#         for j in range(cols):
#             cv2.rectangle(rect_im, (int(i*dx), int(j*dy)), (int((i+1)*dx), int((j+1)*dy)),
#                           thickness=-1,
#                           color=COLORS[cls_idx[j, i].item()])
#     return rect_im


# def label_grid_with_class(img, cls_idx, cls_idx_label, grid_shape):
#     r"""
#     Writes class text name in grid center locations
#     """
#     rect_im = np.copy(img)
#     h, w, _ = rect_im.shape
#     rows, cols = grid_shape
#     dy, dx = h / rows, w / cols
#     for i in range(rows):
#         for j in range(cols):
#             cls_label = cls_idx_label[cls_idx[j, i].item()]
#             cv2.putText(rect_im,
#                         cls_label[:6],
#                         (int((i+0.1)*dx), int((j+0.5)*dy)),
#                         fontFace=cv2.FONT_HERSHEY_SIMPLEX,
#                         fontScale=0.45,
#                         color=(255, 255, 255),
#                         lineType=cv2.LINE_AA)
#     return rect_im

from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import MultiStepLR
import os, random, numpy as np, torch

# adding extras:
import time
import matplotlib.pyplot as plt

def collate_function(data):
    return list(zip(*data))

def train_model(config):
    dataset_config = config['dataset_params']
    model_config = config['model_params']
    train_config = config['train_params']

    seed = train_config['seed']
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        device = torch.device('cuda')
        torch.cuda.manual_seed_all(seed)
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
    else:
        device = torch.device('cpu')

    voc = VOCDataset('train', im_sets=dataset_config['train_im_sets'])
    train_dataset = DataLoader(voc,
                               batch_size=train_config['batch_size'],
                               shuffle=True,
                               collate_fn=collate_function)

    yolo_model = YOLOV1(im_size=dataset_config['im_size'],
                        num_classes=dataset_config['num_classes'],
                        model_config=model_config)
    yolo_model.train()
    yolo_model.to(device)

    if os.path.exists(os.path.join(train_config['task_name'], train_config['ckpt_name'])):
        print('Loading checkpoint as one exists')
        yolo_model.load_state_dict(torch.load(
            os.path.join(train_config['task_name'], train_config['ckpt_name']),
            map_location=device))

    if not os.path.exists(train_config['task_name']):
        os.mkdir(train_config['task_name'])

    optimizer = torch.optim.SGD(lr=train_config['lr'],
                                params=filter(lambda p: p.requires_grad, yolo_model.parameters()),
                                weight_decay=5E-4,
                                momentum=0.9)
    scheduler = MultiStepLR(optimizer, milestones=train_config['lr_steps'], gamma=0.5)
    criterion = YOLOV1Loss()
    acc_steps = train_config['acc_steps']
    num_epochs = train_config['num_epochs']
    steps = 0

    losses_per_epoch = []
    start_time = time.time()

    best_loss = float('inf')
    start_time = time.time()

    print("Training started...")
    for epoch_idx in range(num_epochs):
        losses = []
        optimizer.zero_grad()
        for idx, (ims, targets, _) in enumerate(tqdm(train_dataset, desc=f"Epoch {epoch_idx+1}/{num_epochs}")):
            yolo_targets = torch.cat([
                target['yolo_targets'].unsqueeze(0).float().to(device)
                for target in targets], dim=0)
            im = torch.cat([im.unsqueeze(0).float().to(device) for im in ims], dim=0)
            yolo_preds = yolo_model(im)
            loss = criterion(yolo_preds, yolo_targets, use_sigmoid=model_config['use_sigmoid'])
            loss = loss / acc_steps
            loss.backward()
            losses.append(loss.item())
            if (idx + 1) % acc_steps == 0:
                torch.nn.utils.clip_grad_norm_(yolo_model.parameters(), max_norm=5)
                optimizer.step()
                optimizer.zero_grad()
            if torch.isnan(loss):
                print('Loss is becoming nan. Exiting')
                exit(0)
            steps += 1

        epoch_loss = np.mean(losses)
        losses_per_epoch.append(epoch_loss)
        scheduler.step()

        ckpt_path = os.path.join(train_config['task_name'], f"model_epoch_{epoch_idx+1}.pth")
        torch.save(yolo_model.state_dict(), ckpt_path)

        if epoch_loss < best_loss:
            best_loss = epoch_loss
            best_model_path = os.path.join(train_config['task_name'], 'yolov1.pth')
            torch.save(yolo_model.state_dict(), best_model_path)
            print(f"âœ… Saved new best model at epoch {epoch_idx+1} with loss {best_loss:.4f}")

    total_time = time.time() - start_time
    print(f'\nðŸ•’ Total Training Time: {total_time:.2f} seconds')

    # ðŸ“Š Plot Loss
    plt.figure(figsize=(6, 4))
    plt.plot(range(1, len(losses_per_epoch) + 1), losses_per_epoch, marker='o', linestyle='-')
    plt.xticks(range(1, len(losses_per_epoch) + 1))
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss per Epoch')
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("training_loss_plot.png")
    plt.show()



    print('Done Training...')

"""# **Training data**"""

train_model(config)

from torch.utils.data import DataLoader
import cv2, os, random
import torch

def decode_yolo_output(yolo_pred, S, B, C, use_sigmoid=False):
    out = yolo_pred.reshape((S, S, 5 * B + C))
    if use_sigmoid:
        out[..., :5 * B] = torch.nn.functional.sigmoid(out[..., :5 * B])
    out = torch.clamp(out, min=0., max=1.)
    class_score, class_idx = torch.max(out[..., 5 * B:], dim=-1)

    shifts_x = torch.arange(0, S, dtype=torch.int32, device=out.device) * 1 / float(S)
    shifts_y = torch.arange(0, S, dtype=torch.int32, device=out.device) * 1 / float(S)
    shifts_y, shifts_x = torch.meshgrid(shifts_y, shifts_x, indexing="ij")

    boxes, confidences, labels = [], [], []
    for box_idx in range(B):
        boxes_x1 = ((out[..., box_idx * 5] * 1 / S + shifts_x) -
                    0.5 * torch.square(out[..., 2 + box_idx * 5])).reshape(-1, 1)
        boxes_y1 = ((out[..., 1 + box_idx * 5] * 1 / S + shifts_y) -
                    0.5 * torch.square(out[..., 3 + box_idx * 5])).reshape(-1, 1)
        boxes_x2 = ((out[..., box_idx * 5] * 1 / S + shifts_x) +
                    0.5 * torch.square(out[..., 2 + box_idx * 5])).reshape(-1, 1)
        boxes_y2 = ((out[..., 1 + box_idx * 5] * 1 / S + shifts_y) +
                    0.5 * torch.square(out[..., 3 + box_idx * 5])).reshape(-1, 1)
        boxes.append(torch.cat([boxes_x1, boxes_y1, boxes_x2, boxes_y2], dim=-1))
        confidences.append((out[..., 4 + box_idx * 5] * class_score).reshape(-1))
        labels.append(class_idx.reshape(-1))

    return torch.cat(boxes), torch.cat(confidences), torch.cat(labels)

def inference(config):
    dataset_config = config['dataset_params']
    model_config = config['model_params']
    train_config = config['train_params']

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    voc = VOCDataset('test',
                     im_sets=dataset_config['test_im_sets'],
                     im_size=dataset_config['im_size'],
                     S=model_config['S'],
                     B=model_config['B'],
                     C=dataset_config['num_classes'])
    test_dataset = DataLoader(voc, batch_size=1, shuffle=False)

    yolo_model = YOLOV1(im_size=dataset_config['im_size'],
                        num_classes=dataset_config['num_classes'],
                        model_config=model_config)
    yolo_model.eval().to(device)

    ckpt_path = os.path.join(train_config['task_name'], train_config['ckpt_name'])
    assert os.path.exists(ckpt_path), f"No checkpoint found at {ckpt_path}"
    yolo_model.load_state_dict(torch.load(ckpt_path, map_location=device))

    os.makedirs('samples/preds', exist_ok=True)

    num_samples = 5
    conf_threshold = train_config['infer_conf_threshold']
    conf_threshold = 0.07
    nms_threshold = train_config['nms_threshold']

    for i in range(num_samples):
        dataset_idx = random.randint(0, len(voc) - 1)
        im_tensor, targets, fname = voc[dataset_idx]
        out = yolo_model(im_tensor.unsqueeze(0).to(device))

        boxes, scores, labels = decode_yolo_output(out,
                                                           S=model_config['S'],
                                                           B=model_config['B'],
                                                           C=dataset_config['num_classes'],
                                                           use_sigmoid=model_config['use_sigmoid'])

        keep = torch.where(scores > conf_threshold)[0]
        boxes, scores, labels = boxes[keep], scores[keep], labels[keep]

        keep_mask = torch.zeros_like(scores, dtype=torch.bool)
        for class_id in torch.unique(labels):
            curr_idx = torch.where(labels == class_id)[0]
            curr_keep = torch.ops.torchvision.nms(boxes[curr_idx], scores[curr_idx], nms_threshold)
            keep_mask[curr_idx[curr_keep]] = True

        boxes = boxes[keep_mask]
        scores = scores[keep_mask]
        labels = labels[keep_mask]

        print(f"Image {i}: {len(boxes)} boxes after confidence + NMS")
        if len(scores) > 0:
            print("Top 5 Scores:", scores[:5].tolist())
        else:
            print("âš ï¸ No confident detections.")

        im = cv2.imread(fname)

        im = cv2.imread(fname)
        h, w = im.shape[:2]
        boxes[..., 0::2] *= w
        boxes[..., 1::2] *= h

        if len(boxes) > 0:
            out_img = visualize(image=im,
                                bboxes=boxes.detach().cpu().numpy(),
                                category_ids=labels.detach().cpu().numpy(),
                                category_id_to_name=voc.idx2label,
                                scores=scores.detach().cpu().numpy())
        else:
            # Optional: fallback display
            cv2.putText(im, "No objects detected", (30, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            out_img = im

        cv2.imwrite(f'samples/preds/{i}_pred.jpeg', out_img)

    print('âœ… Inference complete. Check samples/preds/')

"""# **Inference**"""

inference(config)

